%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% LaTeX Example: Project Report
%
% Source: http://www.howtotex.com
%
% Feel free to distribute this example, but please keep the referral
% to howtotex.com
% Date: March 2011 
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% How to use writeLaTeX: 
%
% You edit the source code here on the left, and the preview on the
% right shows you the result within a few seconds.
%
% Bookmark this page and share the URL with your co-authors. They can
% edit at the same time!
%
% You can upload figures, bibliographies, custom classes and
% styles using the files menu.
%
% If you're new to LaTeX, the wikibook is a great place to start:
% http://en.wikibooks.org/wiki/LaTeX
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Edit the title below to update the display in My Documents
%\title{Project Report}
%
%%% Preamble
\documentclass[paper=a4, fontsize=11pt]{scrartcl}
\usepackage[T1]{fontenc}
\usepackage{fourier}

\usepackage[english]{babel}															% English language/hyphenation
\usepackage[protrusion=true,expansion=true]{microtype}	
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage[pdftex]{graphicx}	
\usepackage{url}

\usepackage{enumerate}
\usepackage{babel,blindtext}


%%% Custom sectioning
\usepackage{sectsty}
\allsectionsfont{\centering \normalfont\scshape}


%%% Custom headers/footers (fancyhdr package)
\usepackage{fancyhdr}
\pagestyle{fancyplain}
\fancyhead{}											% No page header
\fancyfoot[L]{}											% Empty 
\fancyfoot[C]{}											% Empty
\fancyfoot[R]{\thepage}									% Pagenumbering
\renewcommand{\headrulewidth}{0pt}			% Remove header underlines
\renewcommand{\footrulewidth}{0pt}				% Remove footer underlines
\setlength{\headheight}{13.6pt}


%%% Equation and float numbering
\numberwithin{equation}{section}		% Equationnumbering: section.eq#
\numberwithin{figure}{section}			% Figurenumbering: section.fig#
\numberwithin{table}{section}				% Tablenumbering: section.tab#


%%% Code presentation
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{color}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegray},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=3
}
\lstset{style=mystyle}


%%% Maketitle metadata
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} 	% Horizontal rule

\title{
		%\vspace{-1in} 	
		\usefont{OT1}{bch}{b}{n}
		\normalfont \normalsize \textsc{Numerical Analysis, ECE 491 / CS 450} \\ [25pt]
		\horrule{0.5pt} \\[0.4cm]
		\huge Final Project Report \\
		\horrule{2pt} \\[0.5cm]
}
\author{
		\normalfont 								\normalsize
        Gregory Linkowski, linkows2\\[-3pt]		\normalsize
        \today
}
\date{}


%%% Begin document
\begin{document}
\maketitle
\section{: Computer Problem 3.5}
Given an elliptical orbit
\begin{align}
	a \, y^2 + b \, x\, y + c\, x + d\, y + e = x^2
\end{align}
and the positional observations below, solve for the parameters using the following specifications:
\begin{table}[!hbt]
	\begin{center}
		\caption{Observational input data for 3.5}
		\label{tab:35input}
		\begin{tabular}{ c|c c c c c }
			x & 1.02 & 0.95 & 0.87 & 0.77 & 0.97 \\
			y & 0.39 & 0.32 & 0.27 & 0.22 & 0.18 \\
			\hline
			x & 0.56 & 0.44 & 0.30 & 0.16 & 0.01 \\
			y & 0.15 & 0.13 & 0.12 & 0.13 & 0.15 \\
		\end{tabular}
	\end{center}
\end{table}

\begin{enumerate}[(a)]
	\item Use least squares to determine the parameters and plot  \vspace{-2mm}
	\item Perturb the data slightly, then plot \vspace{-2mm}
	\item Solve the system with varying tolerance values \vspace{-2mm}
	\item Obtain the Eigenvalues using Singular Value Decomposition \vspace{-2mm}
	\item Solve the system using successively more Eigenvalues, starting with the largest, then the two largest, the three largest, ... \vspace{-2mm}
	\item Perturb the data, then repeat part (e) \vspace{-2mm}
	\item Reformulate as a total least squares problem \vspace{-2mm}
\end{enumerate}

\vspace{4mm}
\subsection{Figures}

\begin{figure}[!hbt]
	\minipage{0.32\columnwidth}
		\includegraphics[width=\linewidth]{figs/use/p01_ptA}
		\caption{Plot of the orbit derived from data}
		\label{fig:3.5-a}
	\endminipage\hfill
	\minipage{0.32\columnwidth}
		\includegraphics[width=\linewidth]{figs/use/p01_ptB-1}
		\caption{One result from perturbed data}
		\label{fig:p3.5-b1}
	\endminipage\hfill
	\minipage{0.32\columnwidth}
		\includegraphics[width=\linewidth]{figs/use/p01_ptB-2}
		\caption{Another perturbed result}
		\label{fig:p3.5-b2}
	\endminipage\hfill
\end{figure}
\begin{figure}[!hbt]
	\minipage{0.48\columnwidth}
		\includegraphics[width=\linewidth]{figs/use/p01_ptE}
		\caption{Plots from SVD approach using k largest Eigenvalues}
		\label{fig:3.5-e}
	\endminipage\hfill
	\minipage{0.48\columnwidth}
		\includegraphics[width=\linewidth]{figs/use/p01_ptF}
		\caption{Plots from SVD approach w/ perturbed data, using k Eigenvalues}
		\label{fig:3.5-f}
	\endminipage\hfill
\end{figure}
\begin{figure}[!hbt]
	\minipage{0.60\columnwidth}
	\includegraphics{figs/use/p01_ptG}
	\caption{Total Least Squares applied to original and perturbed data from part (b)}
	\label{fig:3.5-g}
	\endminipage\hfill
\end{figure}

\vspace{4mm}
\subsection{Qualitative Analysis}

\begin{enumerate}[(a)]
	\item[a) Question] What values are found for the parameters? \vspace{-2mm}
		\item[Answer] $a=-2.636, \; b=0.1437, \; c=0.5515, \; d=3.223, \; e=-0.4329$
	\item[b) Question] What effect does this [random perturbation] have on the orbit? Can you explain this behavior? \vspace{-2mm}
		\item[Answer] Depending on the average change in the original data, the minimization of error can cause the matched orbit to vary significantly. In  Fig.~\ref{fig:3.5-b1}, the middle points are relatively closer to (0.5, 0), causing the orbit to shrink, while in Fig.~\ref{fig:p3.5-b2}, they are relatively closer to the center of the orbit, causing wall of the calculated ellipse to be significantly flatter, leading to a much larger orbit.
	\item[c) Question] What is the resulting rank of the matrix for each value of the tolerance? ... as the tolerance and rank vary? \vspace{-2mm}
		\item[Answer] I tried a few different library routines that claimed to apply a tolerance to singular values, including one that applied SVD, but in no case was I able to change the rank of the matrix.
	\item[c) Question] Which solution would you regard as better: one that fits the data more closely, or one that is less sensitive to small perturbations in the data? Why? \vspace{-2mm}
		\item[Answer] Typically, it would be better to be less sensitive to noise in the data. Especially in the case where the measured data has relatively few significant digits, small changes can magnify the error in the outcome.
	\item[f) Question] What effect does this [random perturbation] have on the plot of the orbits? Can you explain this behavior? \vspace{-2mm}
		\item[Answer] As can be seen in Figs.~\ref{fig:3.5-e} \& ~\ref{fig:3.5-e}, the orbits built from only one or two largest Eigenvalues are too erroneous to be useful. In fact, for $k={1, 2, 3}$, it is difficult to describe the effect of the perturbation. However, for $k=4$, the orbit has grown wider, while for the full $k=5$, it has shrunk. This orbit appears similar to the one in Fig.~\ref{fig:p3.5-b1}, and the explanation is similar: if one considers the mean squared error as a sort of center of gravity of the points, then as that center moves outwards, it skews towards a tighter orbit.
	\item[f) Question] Which solution would you regard as better: one that fits the data more closely, or one that is less sensitive to small perturbations in the data? Why? \vspace{-2mm}
		\item[Answer] Similarly to (c), it would generally be good to have some tolerance of noise in the data. At the same time, one would want as true a fit as possible. So the best approach is probably to have a high tolerance for noise, and then collect as many data points as possible in an effort to reduce the mean effect of the noise.
	\item[g)] Regarding the question of fitting noisy data, the total least-squares approach in Fig.~\ref{fig:3.5-g} does a better job of fitting to the noisy data from part (b) that resulted in the abnormally large orbit.
\end{enumerate}

\vspace{4mm}
\subsection{Terminal Output}
\begin{verbatim}
>>>> Part A >>>>
Parameters: a=-2.6356, b=0.14365, c=0.55145, d=3.2229, e=-0.43289

>>>> Part B >>>>
Parameters: a=0.019864, b=-1.128, c=0.7277, d=3.1294, e=-0.46705

>>>> Part C >>>>
condition number of original matrix A: 688.4294468693159
Parameters: a=-2.6356, b=0.14365, c=0.55145, d=3.2229, e=-0.43289
tolerance 10^-12, rank(A^+) = 4
Mean Rel. error of Y observed vs calc: 1.219
tolerance 10^-6, rank(A^+) = 4
Mean Rel. error of Y observed vs calc: 1.219
tolerance 10^-3, rank(A^+) = 4
Mean Rel. error of Y observed vs calc: 1.219
tolerance 10^-1, rank(A^+) = 4
Mean Rel. error of Y observed vs calc: 1.219
tolerance 10^1, rank(A^+) = 4
Mean Rel. error of Y observed vs calc: 1.219
tolerance 10^3, rank(A^+) = 4
Mean Rel. error of Y observed vs calc: 1.219

>>>> Part D >>>>
Resulting eigen values: [ 3.78603638  0.94492272  0.20891299  0.02304315  0.00549953]

>>>> Part E >>>>
Parameters for ...
k=1:    a=0.018956, b=0.054727, c=0.21173, d=0.073671, e=0.33612
k=2:    a=0.10442, b=0.34014, c=0.89179, d=0.20249, e=-0.17179
k=3:    a=0.41916, b=0.87687, c=0.52535, d=0.75614, e=-0.16745
k=4:    a=-0.65279, b=-0.94511, c=0.71682, d=3.261, e=-0.47997
k=5:    a=-2.6356, b=0.14365, c=0.55145, d=3.2229, e=-0.43289

>>>> Part F >>>>
Resulting eigen values: [ 3.78472235  0.94249581  0.20531615  0.02398268  0.00531986]
Parameters for ...
k=1:    a=0.018802, b=0.054397, c=0.21122, d=0.07351, e=0.33571
k=2:    a=0.10254, b=0.33982, c=0.89651, d=0.19851, e=-0.17375
k=3:    a=0.42863, b=0.89833, c=0.51849, d=0.78225, e=-0.1725
k=4:    a=-0.42657, b=-0.73179, c=0.70291, d=2.9397, e=-0.44891
k=5:    a=-2.1607, b=0.17261, c=0.56266, d=2.9522, e=-0.41283

>>>> Part G >>>>

\end{verbatim}

\vspace{4mm}
\section{: Computer Problem 3.8}
Create an ill-conditioned least squares problem with a small residual and fit it to the polynomial
\begin{align}
	p_{n-1}(t) &= x_1 + x_2 t + x_3 t^2 + \dots + x_n t^{n-1}
\end{align}
and compare Cholesky factorization to QR factorization for solving the problem.

\vspace{4mm}
\subsection{Qualitative Analysis}

\begin{enumerate}[(a)]
	\item[Question] For which method is the solution more sensitive to the perturbation we introduced into the data? \vspace{-2mm}
		\item[Answer] In most cases, the Cholesky factorization had a slightly less mean error in the recovered $x$ vector (with $x_j$ entries) after perturbing the data. A notable exception was the final attempt, where the original $x$ vector was created from random noise.
	\item[Question] Which method comes closer to recovering the $x$ that we used to generate the data? \vspace{-2mm}
		\item[Answer] On unperturbed data, QR factorization came much closer to recovering the original $x$ vector than Cholesky, typically with a mean accuracy of up to nine decimal places.
	\item[Question] Does the fact that the solutions differ affect our ability to fit the data points $(t_i, y_i)$ closely by the polynomial? Why? \vspace{-2mm}
		\item[Answer] No. The most limiting factor in fitting the data is the nature of the Monomial basis created by the polynomial. Also, the introduction of evenly-distributed noise into the data points should have a relatively small result on the fit, if there are enough data points, as the fit tries to minimize the error across all the points, effectively smoothing the noise.
\end{enumerate}

\vspace{4mm}
\subsection{Terminal Output}
\begin{verbatim}
----------------------------------------------------------
Results for xj = 1
Using QR factorization, rel error = 4.194813718605417e-09
applied to perturbed data, error = 6.968947058103548

Using Cholesky,    relative error = 0.16297710364142834
applied to perturbed data, error = 7.036566937724132

----------------------------------------------------------
Results for xj = 2
Using QR factorization, rel error = 4.194813718605417e-09
applied to perturbed data, error = 0.9430992054072757

Using Cholesky,    relative error = 0.16297710364142834
applied to perturbed data, error = 0.5812272652237548

----------------------------------------------------------
Results for xj = -1.5
Using QR factorization, rel error = 1.854586237668185e-09
applied to perturbed data, error = 0.5017636159005733

Using Cholesky,    relative error = 0.017440850176803257
applied to perturbed data, error = 0.40578659794847444

----------------------------------------------------------
Results for xj = [ 1  2  3  4  5  6  7  8  9 10 11 12]
Using QR factorization, rel error = 3.3553251170972185e-09
applied to perturbed data, error = 0.20474350842647604

Using Cholesky,    relative error = 0.1310150122172821
applied to perturbed data, error = 0.0634397507887568

----------------------------------------------------------
Results for xj = [-0.69368289  0.60957607  0.16030713 -1.5216443   0.40465164 -1.3739347
1.55291871  1.81075632 -0.13306472 -0.51321109 -1.41987016 -1.86089748]
Using QR factorization, rel error = 2.244652684976526e-09
applied to perturbed data, error = 0.43993053692418854

Using Cholesky,    relative error = 0.4816971219755919
applied to perturbed data, error = 0.7709796869516125
\end{verbatim}

\vspace{4mm}
\section{: Computer Problem 3.13}
What is the exact solution x to the following linear least squares problem as a function of $\epsilon$?
\begin{align}
	\begin{bmatrix}
		1 & 1 & 1 \\ \epsilon & 0 & 0 \\ 0 & \epsilon & 0 \\ 0 & 0 & \epsilon
	\end{bmatrix}
	\begin{bmatrix}
		x_1 \\ x_2 \\ x_3
	\end{bmatrix}
	&\approx
	\begin{bmatrix}
		1 \\ 0 \\ 0 \\ 0
	\end{bmatrix}
\end{align}
Solve the system using each of the following methods while experimenting with $\epsilon$:
\begin{enumerate}[(a)]
	\item Normal equations \vspace{-2mm}
	\item Augmented system \vspace{-2mm}
	\item Householder QR \vspace{-2mm}
	\item Givens QR \vspace{-2mm}
	\item Classical Gram-Schmidt orthogonalization \vspace{-2mm}
	\item Modified Gram-Schmidt orthogonalization \vspace{-2mm}
	\item Classical Gram-Schmidt with iterative refinement \vspace{-2mm}
	\item Singular value decomposition \vspace{-2mm}
\end{enumerate}

\vspace{4mm}
\subsection{Qualitative Analysis}
What is the exact solution for $x_i$ as a function of $\epsilon$?
\begin{align}
	x_i &= \frac{1}{e^2 + 3}  \label{eq:3.8-truex} \\
	\textnormal{where \hspace{5mm}} x &= [x_i, x_i, x_i]^T	
\end{align}
\par Error for each calculation was found by comparing the resulting $x$ vector against that calculated in Eq.~\ref{eq:3.8-truex}. As seen in Fig.~\ref{fig:3.8}, error as a function of $\epsilon$ held fairly steady until the last three entries, which represent values approaching $\sqrt{\epsilon_{mach}} \; \& \; \epsilon_{mach}$. (Smaller values were attempted but the methods completely broke down.) As can be seen, error jumped significantly at the bottom values for most of the methods. The three strongest methods appear to be the Augmented system, Householder QR, and SVD. \\ 

\vspace{4mm}
\subsection{Data Output to File}
Error values were output to a file, displayed in Fig.~\ref{fig:3.8}.
\begin{center}
	\begin{figure}[!h]
		\includegraphics[width=\columnwidth]{figs/use/p03_table}
		\caption{Table of results output from 3.8: error for each method in calculated value of $x_i$ as a function of $\epsilon$. Error displayed is the sum of the relative error for each calculated $x_i$ vs the $x_i$ found in Eq.~\ref{eq:3.8-truex} }
		\label{fig:3.8}
	\end{figure}
\end{center}

\vspace{4mm}
\subsection{Terminal Output}
\begin{verbatim}
Using epsilon = 5.0
x as a function of epsilon: [ 0.03571429  0.03571429  0.03571429]
Relative error ...
Normal:      1.49e-07         Augmented: 1.34e-07
Householder: 7.75e-07         Givens:    0.00e+00
G-S Classic: 1.49e-07         G-S Mod:   1.49e-07
G-S Iter:    1.49e-07         SVD:       1.34e-07

Using epsilon = 3.0
x as a function of epsilon: [ 0.08333333  0.08333333  0.08333333]
Relative error ...
Normal:      1.49e-07         Augmented: 8.94e-08
Householder: 5.66e-07         Givens:    0.00e+00
G-S Classic: 1.49e-07         G-S Mod:   1.49e-07
G-S Iter:    1.49e-07         SVD:       1.79e-07

Using epsilon = 2.0
x as a function of epsilon: [ 0.14285714  0.14285714  0.14285714]
Relative error ...
Normal:      1.64e-07         Augmented: 1.34e-07
Householder: 3.43e-07         Givens:    0.00e+00
G-S Classic: 2.68e-07         G-S Mod:   2.68e-07
G-S Iter:    2.68e-07         SVD:       1.34e-07

Using epsilon = 1.0
x as a function of epsilon: [ 0.25  0.25  0.25]
Relative error ...
Normal:      1.19e-07         Augmented: 0.00e+00
Householder: 2.98e-07         Givens:    0.00e+00
G-S Classic: 2.38e-07         G-S Mod:   4.17e-07
G-S Iter:    2.38e-07         SVD:       1.79e-07

Using epsilon = 0.5
x as a function of epsilon: [ 0.30769231  0.30769231  0.30769231]
Relative error ...
Normal:      3.28e-07         Augmented: 1.12e-07
Householder: 5.22e-07         Givens:    0.00e+00
G-S Classic: 8.34e-07         G-S Mod:   1.10e-06
G-S Iter:    8.34e-07         SVD:       1.79e-07

Using epsilon = 0.05
x as a function of epsilon: [ 0.33305579  0.33305579  0.33305579]
Relative error ...
Normal:      6.35e-05         Augmented: 6.66e-08
Householder: 6.66e-08         Givens:    0.00e+00
G-S Classic: 2.14e-05         G-S Mod:   4.28e-05
G-S Iter:    2.14e-05         SVD:       6.66e-08

Using epsilon = 0.005
x as a function of epsilon: [ 0.33333056  0.33333056  0.33333056]
Relative error ...
Normal:      5.00e-05         Augmented: 1.24e-07
Householder: 3.09e-07         Givens:    0.00e+00
G-S Classic: 2.77e-03         G-S Mod:   5.53e-03
G-S Iter:    2.77e-03         SVD:       1.24e-07

Using epsilon = 0.00048828125
x as a function of epsilon: [ 0.33333331  0.33333331  0.33333331]
Relative error ...
Normal:      7.35e-07         Augmented: 5.96e-08
Householder: 2.88e-07         Givens:    0.00e+00
G-S Classic: 6.46e-07         G-S Mod:   9.14e-07
G-S Iter:    6.46e-07         SVD:       2.09e-07

Using epsilon = 0.00034526698300124393
x as a function of epsilon: [ 0.33333332  0.33333332  0.33333332]
Relative error ...
Normal:      2.00e+00         Augmented: 5.96e-08
Householder: 1.09e-07         Givens:    0.00e+00
G-S Classic: 4.00e+00         G-S Mod:   4.00e+00
G-S Iter:    4.00e+00         SVD:       5.96e-08

Using epsilon = 0.00024526698300124394
x as a function of epsilon: [ 0.33333333  0.33333333  0.33333333]
Relative error ...
Normal:      2.00e+00         Augmented: 1.19e-07
Householder: 2.29e-07         Givens:    0.00e+00
G-S Classic: 4.00e+00         G-S Mod:   4.00e+00
G-S Iter:    4.00e+00         SVD:       1.19e-07
\end{verbatim}


\vspace{4mm}
\section{: Computer Problem 6.13}
Find least squares solutions to the following overdetermined systems of nonlinear equations:
\begin{enumerate}[(a)]
	\item \begin{align*}
		x_1^2 + x_2^2 \; &= \; 2 \\
		(x_1 - 2)^2 + x_2^2 \; &= \; 2 \\
		(x_1 - 1)^2 + x_2^2 \; &= \; 9
	\end{align*}
	\item \begin{align*}
		x_1^2 + x_2^2 + x_1 x_2 \; &= \; 0 \\
		sin^2(x_1) \; &= \; 0 \\
		cos^2(x_2) \; &= \; 0
	\end{align*}
\end{enumerate}

\vspace{4mm}
\subsection{Qualitative Analysis}
\par As seen in the plots (Fig.~\ref{fig:6.13plotA}, Fig.~\ref{fig:6.13plotB}), the equations in the system don't all overlap at a single point, if at all. Two methods were used to attempt to find the optimal solution for $[x_1, \; x_2]^T$: a hand-coded implementation of the Gauss-Newton method, and the SciPy minimize routine, with a default implementation of BFGS. The equations were rearranged such that the expected result was a zero vector, thus the residual minus the result was simply $-f(x)$. \\
\par The routines were each run with different $x_0$ starting vectors, and the results found for part (a) were identical. However, for part (b), one of the results diverged significantly between the Gauss-Newton and SciPy implementations. It should be noted that the Gauss-Newton implementation had difficulty converging for part (b), unless some non-zero value was added to the right-hand side of the equations. For instance, when all equations were set equal to 0.5, both implementations had the same results. \\
\par The results are easy to explain for part (a), in that the optimal solutions were found midway between the point where the two inner circles meet, and the outer circle (see Fig.~\ref{fig:6.13plotA}). Explaining part (b) is more difficult, as one might reasonably expect the minimal optimum to be $[x_1, \; x_2]^T = [0, \; \pm \frac{\pi}{4}]^T$, as that would lie midway between (0, 0) and (0, $\pm \frac{\pi}{2}$). However, it appears the optimum is influenced by the angle of the ellipse, even as it approaches its limit. \\


\vspace{4mm}
\subsection{Figures}
\par The following figures show plots of the given equations. Note that in Fig.~\ref{fig:6.17plotB}, as the values on the right of the equations approach zero, the coordinates approach (0,0), (0,$x_2$), ($x_1$,$\frac{\pi}{2}$), respectively. \\
\begin{figure}[!hbt]
	\minipage{0.55\columnwidth}
		\includegraphics[width=\linewidth]{figs/use/p04_plotA}
		\caption{Over-determined curves for part (a)}
		\label{fig:6.13plotA}
	\endminipage\hfill
	\minipage{0.44\columnwidth}
		\includegraphics[width=\linewidth]{figs/use/p04_plotB2}
		\caption{Over-determined curves for part (b), shown progressively approaching zero.}
		\label{fig:6.13plotB}
	\endminipage\hfill
\end{figure}

\vspace{4mm}
\subsection{Terminal Output}
\begin{verbatim}
>>>> Part A >>>>

Newton results ---------------------
Newton iterations: 5, delta: 3.14e-16
using x0 = [2, 2]
found x = [1.0000, 1.9149]
result f(x) = [2.6667, 2.6667, -5.3333]

minimize from SciPy library --------
using x0 = [2, 2]
found x = [1.0000, 1.9149]
result f(x) = [2.6667, 2.6667, -5.3333]

Newton results ---------------------
Newton iterations: 7, delta: 2.22e-16
using x0 = [-1, -1]
found x = [1.0000, -1.9149]
result f(x) = [2.6667, 2.6667, -5.3333]

minimize from SciPy library --------
using x0 = [-1, -1]
found x = [1.0000, -1.9149]
result f(x) = [2.6667, 2.6667, -5.3333]


>>>> Part B >>>>

Newton results ---------------------
Newton iterations: 64, delta: 1.54e+00
using x0 = [2, 2]
found x = [0.1410, 1.6200]
result f(x) = [0.6443, 4.0804, -5.6376]

minimize from SciPy library --------
using x0 = [2, 2]
found x = [0.3149, -0.7593]
result f(x) = [-1.3244, 1.4160, -7.9542]

Newton results ---------------------
Newton iterations: 64, delta: 2.26e-01
using x0 = [1, -1]
found x = [0.4723, -0.7812]
result f(x) = [-1.1667, 0.9442, -8.1112]

minimize from SciPy library --------
using x0 = [1, -1]
found x = [0.3149, -0.7593]
result f(x) = [-1.3244, 1.4160, -7.9542]
\end{verbatim}


\vspace{4mm}
\section{: Computer Problem 6.16}
Given the Michaelis-Menten equation
\begin{align*}
	v_0 &= \frac{V}{1 + K_m/S}
\end{align*}
and the data below, try to optimize $V$ and $K_m$ to fit the measured data. Then, for part (b), compare several algebraic rearrangements that reformulate the equation into a linear least squares problem.
\begin{table}[!hbt]
	\begin{center}
		\caption{Measured input data for 6.16}
		\label{tab:6.16input}
		\begin{tabular}{ c|c c c }
			S & 2.5 & 5.0 & 10.0\\
			$v_0$ & 0.024 & 0.036 & 0.053 \\
			\hline
			S & 15.0 & 20.0 & \\
			$v_0$ & 0.060 & 0.064 & \\
		\end{tabular}
	\end{center}
\end{table}

\vspace{4mm}
\subsection{Qualitative Analysis}
\par For part (a), the variables $V \; \& \; K_m$ were placed into the vector $x = [V, \; K_m]^T$, which was passed to the equation. The residual was defined as $v_0 - f(S, x)$. These were used along with the 5x2 Jacobian so solve the Gauss-Newton optimization. Values were found for $x$ which best fit the data in Tab.~\ref{tab:6.16input}. These found values were then passed back into the equation and the $L_2$ norm between the original and resulting $v_0$ was calculated as a measure of the difference. Another measure was the maximum relative difference between the measured and calculated $v_0$. \\
\par As seen in the output below, the resulting $v_0(S)$ contained one entry that differed by 13\% from the original, while the other entries were much closer. The normed distance between the two vectors was 0.009, suggesting that the results are fairly accurate, given the potential for error in the measured data. \\
\par In part (b), the equation was rearranged to remove the non-linearity. The resulting equations were solved using a typical linear least-squares approach. The difference between the values found through the linear method and found with the non-linear optimization were output as relative error for each rearranged equation. \\
\par As can be seen in the first two rearrangements, the variables matched the first digit but typically not the second, with error up to 6.3\%. However, the third rearrangement (Eadie \& Hofstee) performed notably better, with differences of under 0.3\%. For the provided data, much of which has no more than two significant digits, the Eadie \& Hofstee linearization is acceptable. \\ 

\vspace{4mm}
\subsection{Terminal Output}
\begin{verbatim}
>>>> Part A >>>>

Newton results -----------------------
Newton iterations: 12, delta: 8.88e-16
using x0 = [2, 2]
found x  = [0.07610, 5.34854]
final v0(S) = [ 0.02423932  0.03676717  0.0600408   0.05609542  0.0600408 ]
diff b/t this and expected: 0.00901
max relative error in v0(S): 0.13285
final:
V = 0.07609734198278333
Km = 5.348544452628809


>>>> Part B >>>>

Lineweaver & Burk rearrangement --------
V = 0.07596,  relative error = 1.84e-03
Km = 5.42706,  relative error = 1.47e-02

Dixon rearrangement --------------------
V = 0.07401,  relative error = 2.74e-02
Km = 5.01001,  relative error = 6.33e-02

Eadie & Hofstee rearrangement ----------
V = 0.07591,  relative error = 2.50e-03
Km = 5.34840,  relative error = 2.67e-05
\end{verbatim}


\vspace{4mm}
\section{: Computer Problem 6.17}
Fit the model function
\begin{align*}
	f(t,x) &= x_1 + x_2 t + x_3 t^2 + x_4 e^{x_5 t}
\end{align*}
to the data below (Tab.~\ref{tab:6.17input}). Using five different approaches, find the parameters that best fit the data. Note that the function is linear in four of the five parameters, and non-linear in the fifth.
\begin{table}[!hbt]
	\begin{center}
		\caption{Observed input data for 6.17}
		\label{tab:6.17input}
		\begin{tabular}{ c|c c c }
			\hline
			t & 0.00 & 0.25 & 0.50 \\
			y & 20.00 & 51.58 & 68.73 \\
			\hline
			t & 0.75 & 1.00 & 1.25 \\
			y & 75.46 & 74.36 & 67.09 \\
			\hline
			t & 1.50 &1.75 & 2.00 \\
			y & 54.73 & 37.98 & 17.28 \\
			\hline
		\end{tabular}
	\end{center}
\end{table}

\vspace{4mm}
\subsection{Qualitative Analysis}
\par Part (a) defined a single-dimensional cost function, referred to alternately in the book as $g(x)$ or $\phi(x)$. Similar to a vector norm, this reduced the residual vector to a single value, and a routine was used to minimize $g(x)$, given a vector $x$. For comparison with the other methods, the resulting $f(t,x)$ was compared with the original data $y$ to find the maximum relative error. This method performed reasonably well, with a maximum error of 4.60\%. \\
\par In part (b), the cost function was replaced with a vector gradient, and a routine was used to find $\nabla g(x) = 0$, defined as $\nabla g(x) = J_f^T(x)\,r(x)$, a combination of the Jacobian and residual of $f(t,x)$. Interestingly, this approach performed less well, with a maximum relative error of 24.2\%. \\
\par Part (c) reduced the problem input to the single non-linear component $x_5$. Given $x_5$, first a linear system was solved for the other four parameters, then to these was applied the cost function from part (a). This improved greatly on the results from part (a), with a maximum relative error of 0.00310\%. \\
\par Similarly to part (c), part (d) took the single element $x_5$, used linear least squares to determine the other elements, then tried to find the root of the gradient function from part (b). Again, the error improved significantly, though not as drastically, resulting in 6.37\%. \\
\par Part (e) applied the Gauss-Newton approach, iteratively solving the least-squares problem $J_f(x_x) \, s_k \approx r(x_k)$, and adjusting $x_{k+1} = x_k + s_k$. This method was the second-poorest of the five, with a maximum relative error in the output of 11.0\%. In this case, every entry of $f(t,x)$ except the first was off by an absolute magnitude of almost 2.0. \\
\par For all five parts, the same starting vector $x_0 = [5, 4, 3, 2, 1]^T$ was used. \\

\vspace{4mm}
\subsection{Terminal Output}
\begin{verbatim}
>>>> Part A >>>>

with objective function g(x) ------------------------
using x0 = [5, 4, 3, 2, 1]
found x  = [-2433.20, -619.19, -217.82, 2454.12, 0.31]
final y = f(t,x) = [20.92, 50.18, 68.13, 76.06, 75.35, 67.51, 54.13, 36.99, 17.95]
max relative error (y, f(t,x)):  4.60 %


>>>> Part B >>>>

with estimated gradient of g(x) ---------------------
using x0 = [5, 4, 3, 2, 1]
found x  = [20.84, 104.92, -60.66, 4.00, 0.94]
final y = f(t,x) = [24.84, 48.34, 64.53, 73.50, 75.33, 70.14, 58.08, 39.35, 14.17]
max relative error (y, f(t,x)):  24.21 %


>>>> Part C >>>>

with linear least squares ---------------------------
using x0 = [1]
found x  = [109.73, 5.23, -25.05, -89.73, -1.75]
final y = f(t,x) = [20.00, 51.58, 68.73, 75.46, 74.36, 67.09, 54.73, 37.98, 17.28]
max relative error (y, f(t,x)):  0.00310 %


>>>> Part D >>>>

with estimated gradient of g(x) ---------------------
using x0 = [1]
found x  = [-12.35, 102.25, -105.63, 33.62, 1.00]
final y = f(t,x) = [21.27, 49.79, 67.81, 76.11, 75.68, 67.79, 54.06, 36.61, 18.10]
max relative error (y, f(t,x)):  6.37 %


>>>> Part E >>>>

by Gauss-Newton method ------------------------------
using x0 = [5, 4, 3, 2, 1]
found x  = [35.27, 87.03, -48.48, -15.27, -6971.41]
final y = f(t,x) = [20.00, 53.99, 66.66, 73.27, 73.81, 68.29, 56.72, 39.08, 15.38]
max relative error (y, f(t,x)):  10.97 %
\end{verbatim}



\pagebreak
\section{Appendix: Code}

\subsection{Computer Problem 3.5}
\lstinputlisting[language=Python]{prob01.py}

\pagebreak
\subsection{Computer Problem 3.8}
\lstinputlisting[language=Python]{prob02.py}

\pagebreak
\subsection{Computer Problem 3.13}
\lstinputlisting[language=Python]{prob03.py}

\pagebreak
\subsection{Computer Problem 6.13}
\lstinputlisting[language=Python]{prob04.py}

\pagebreak
\subsection{Computer Problem 6.16}
\lstinputlisting[language=Python]{prob05.py}

\pagebreak
\subsection{Computer Problem 6.17}
\lstinputlisting[language=Python]{prob06.py}










%TODO: appendix w/ all code [python] & coloring?
% https://www.sharelatex.com/learn/Code_listing

%%% End document
\end{document}